vtune: Peak bandwidth measurement started.
vtune: Peak bandwidth measurement finished.
vtune: Collection started. To stop the collection, either press CTRL-C or enter from another console window: vtune -r /scratch/tsgoncalves/perf-analysis/stage2/results/com-Friendster/performance-snapshot/threads-88/ht-true/bind-close/run-3 -command stop.
Read Time:           0.26032
Build Time:          0.41111
Graph has 124736033 nodes and 1442970 directed edges for degree: 0
Trial Time:          0.39831
Trial Time:          0.39815
Trial Time:          0.35932
Trial Time:          0.35929
Trial Time:          0.45347
Trial Time:          0.37997
Trial Time:          0.39596
Trial Time:          0.37740
Trial Time:          0.37616
Trial Time:          0.48140
Trial Time:          0.37341
Trial Time:          0.38718
Trial Time:          0.37564
Trial Time:          0.37289
Trial Time:          0.38046
Trial Time:          0.37388
Average Time:        0.39018
vtune: Collection stopped.
vtune: Using result path `/scratch/tsgoncalves/perf-analysis/stage2/results/com-Friendster/performance-snapshot/threads-88/ht-true/bind-close/run-3'
vtune: Executing actions  0 %                                                  vtune: Executing actions  0 % Finalizing results                               vtune: Executing actions  0 % Finalizing the result                            vtune: Executing actions  0 % Clearing the database                            vtune: Executing actions  7 % Clearing the database                            vtune: Executing actions  7 % Loading raw data to the database                 vtune: Executing actions  7 % Loading 'systemcollector-81192-blaise.sc' file   vtune: Executing actions 12 % Loading 'systemcollector-81192-blaise.sc' file   vtune: Executing actions 12 % Loading '81202.stat.perf' file                   vtune: Executing actions 12 % Updating precomputed scalar metrics              vtune: Executing actions 14 % Updating precomputed scalar metrics              vtune: Executing actions 14 % Processing profile metrics and debug information vtune: Executing actions 19 % Processing profile metrics and debug information vtune: Executing actions 19 % Setting data model parameters                    vtune: Executing actions 19 % Resolving module symbols                         vtune: Executing actions 19 % Resolving thread name information                vtune: Executing actions 21 % Resolving thread name information                vtune: Executing actions 21 % Resolving call target names for dynamic code     vtune: Executing actions 24 % Resolving call target names for dynamic code     vtune: Executing actions 24 % Resolving interrupt name information             vtune: Executing actions 26 % Resolving interrupt name information             vtune: Executing actions 26 % Processing profile metrics and debug information vtune: Executing actions 28 % Processing profile metrics and debug information vtune: Executing actions 29 % Processing profile metrics and debug information vtune: Executing actions 30 % Processing profile metrics and debug information vtune: Executing actions 31 % Processing profile metrics and debug information vtune: Executing actions 31 % Preparing output tree                            vtune: Executing actions 31 % Parsing columns in input tree                    vtune: Executing actions 32 % Parsing columns in input tree                    vtune: Executing actions 32 % Creating top-level columns                       vtune: Executing actions 32 % Creating top-level rows                          vtune: Executing actions 33 % Creating top-level rows                          vtune: Executing actions 33 % Preparing output tree                            vtune: Executing actions 33 % Parsing columns in input tree                    vtune: Executing actions 33 % Creating top-level columns                       vtune: Executing actions 34 % Creating top-level columns                       vtune: Executing actions 34 % Creating top-level rows                          vtune: Executing actions 35 % Creating top-level rows                          vtune: Executing actions 35 % Setting data model parameters                    vtune: Executing actions 35 % Precomputing frequently used data                vtune: Executing actions 35 % Precomputing frequently used data                vtune: Executing actions 36 % Precomputing frequently used data                vtune: Executing actions 37 % Precomputing frequently used data                vtune: Executing actions 38 % Precomputing frequently used data                vtune: Executing actions 39 % Precomputing frequently used data                vtune: Executing actions 40 % Precomputing frequently used data                vtune: Executing actions 41 % Precomputing frequently used data                vtune: Executing actions 41 % Updating precomputed scalar metrics              vtune: Executing actions 42 % Updating precomputed scalar metrics              vtune: Executing actions 42 % Discarding redundant overtime data               vtune: Executing actions 44 % Discarding redundant overtime data               vtune: Executing actions 44 % Saving the result                                vtune: Executing actions 46 % Saving the result                                vtune: Executing actions 48 % Saving the result                                vtune: Executing actions 49 % Saving the result                                vtune: Executing actions 50 % Saving the result                                vtune: Executing actions 50 % Generating a report                              vtune: Executing actions 50 % Setting data model parameters                    vtune: Executing actions 75 % Setting data model parameters                    vtune: Executing actions 75 % Generating a report                              Elapsed Time: 7.992s
    IPC: 0.191
     | The IPC may be too low. This could be caused by issues such as memory
     | stalls, instruction starvation, branch misprediction or long latency
     | instructions. Explore the other hardware-related metrics to identify what
     | is causing low IPC.
     |
    SP GFLOPS: 3.247
    DP GFLOPS: 1.009
    x87 GFLOPS: 0.001
    Average CPU Frequency: 2.802 GHz 
Effective Logical Core Utilization: 78.6% (69.208 out of 88)
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization. Consider improving physical core utilization as the first step
 | and then look at opportunities to utilize logical cores, which in some cases
 | can improve processor throughput and overall performance of multi-threaded
 | applications.
 |
    Effective Physical Core Utilization: 79.2% (34.849 out of 44)
     | The metric value is low, which may signal a poor physical CPU cores
     | utilization caused by:
     |     - load imbalance
     |     - threading runtime overhead
     |     - contended synchronization
     |     - thread/process underutilization
     |     - incorrect affinity that utilizes logical cores instead of physical
     |       cores
     | Explore sub-metrics to estimate the efficiency of MPI and OpenMP
     | parallelism or run the Locks and Waits analysis to identify parallel
     | bottlenecks for other parallel runtimes.
     |
Microarchitecture Usage: 12.3% of Pipeline Slots
 | You code efficiency on this platform is too low.
 | 
 | Possible cause: memory stalls, instruction starvation, branch misprediction
 | or long latency instructions.
 | 
 | Next steps: Run Microarchitecture Exploration analysis to identify the cause
 | of the low microarchitecture usage efficiency.
 |
    Retiring: 12.3% of Pipeline Slots
    Front-End Bound: 15.5% of Pipeline Slots
    Back-End Bound: 72.6% of Pipeline Slots
     | A significant portion of pipeline slots are remaining empty. When
     | operations take too long in the back-end, they introduce bubbles in the
     | pipeline that ultimately cause fewer pipeline slots containing useful
     | work to be retired per cycle than the machine is capable to support. This
     | opportunity cost results in slower execution. Long-latency operations
     | like divides and memory operations can cause this, as can too many
     | operations being directed to a single execution port (for example, more
     | multiply operations arriving in the back-end per cycle than the execution
     | unit can support).
     |
        Memory Bound: 67.4% of Pipeline Slots
         | The metric value is high. This can indicate that the significant
         | fraction of execution pipeline slots could be stalled due to demand
         | memory load and stores. Use Memory Access analysis to have the metric
         | breakdown by memory hierarchy, memory bandwidth information,
         | correlation by memory objects.
         |
        Core Bound: 5.2% of Pipeline Slots
    Bad Speculation: 0.0% of Pipeline Slots
Memory Bound: 67.4% of Pipeline Slots
 | The metric value is high. This can indicate that the significant fraction of
 | execution pipeline slots could be stalled due to demand memory load and
 | stores. Use Memory Access analysis to have the metric breakdown by memory
 | hierarchy, memory bandwidth information, correlation by memory objects.
 |
    L1 Bound: 14.7% of Clockticks
     | This metric shows how often machine was stalled without missing the L1
     | data cache. The L1 cache typically has the shortest latency. However, in
     | certain cases like loads blocked on older stores, a load might suffer a
     | high latency even though it is being satisfied by the L1.
     |
    L2 Bound: 28.7% of Clockticks
     | This metric shows how often machine was stalled on L2 cache. Avoiding
     | cache misses (L1 misses/L2 hits) will improve the latency and increase
     | performance.
     |
    L3 Bound: 2.6% of Clockticks
    DRAM Bound: 16.3% of Clockticks
     | This metric shows how often CPU was stalled on the main memory (DRAM).
     | Caching typically improves the latency and increases performance.
     |
        DRAM Bandwidth Bound: 0.0% of Elapsed Time
    Store Bound: 18.5% of Clockticks
    NUMA: % of Remote Accesses: 31.5%
     | A significant amount of DRAM loads were serviced from remote DRAM.
     | Wherever possible, try to consistently use data on the same core, or at
     | least the same package, as it was allocated on.
     |
Vectorization: 0.0% of Packed FP Operations
 | A significant fraction of floating point arithmetic instructions are scalar.
 | Use Intel Advisor to see possible reasons why the code was not vectorized.
 |
    Instruction Mix
        SP FLOPs: 6.8% of uOps
            Packed: 0.0% from SP FP
                128-bit: 0.0% from SP FP
                256-bit: 0.0% from SP FP
            Scalar: 100.0% from SP FP
             | This code has floating point operations and is not vectorized.
             | Consider using Intel Advisor to vectorize the loops.
             |
        DP FLOPs: 2.1% of uOps
            Packed: 0.0% from DP FP
                128-bit: 0.0% from DP FP
                256-bit: 0.0% from DP FP
            Scalar: 100.0% from DP FP
             | This code has floating point operations and is not vectorized.
             | Consider using Intel Advisor to vectorize the loops.
             |
        x87 FLOPs: 0.0% of uOps
        Non-FP: 91.1% of uOps
    FP Arith/Mem Rd Instr. Ratio: 0.525
    FP Arith/Mem Wr Instr. Ratio: 1.418
Collection and Platform Info
    Application Command Line: /scratch/tsgoncalves/perf-analysis/stage2/src/gapbs/pr "-f" "./data/com-Friendster/com-Friendster.el" "-i" "500" "-t" "1e-6" 
    Operating System: 4.19.0-25-amd64 10.13
    Computer Name: blaise
    Result Size: 3.7 MB 
    Collection start time: 07:01:03 03/10/2025 UTC
    Collection stop time: 07:01:11 03/10/2025 UTC
    Collector Type: Driverless Perf per-process counting
    CPU
        Name: Intel(R) Xeon(R) Processor code named Broadwell
        Frequency: 2.200 GHz 
        Logical CPU Count: 88
        Max DRAM Single-Package Bandwidth: 98.000 GB/s
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

Recommendations:
    Hotspots: Start with Hotspots analysis to understand the efficiency of your algorithm.
     | Use Hotspots analysis to identify the most time consuming functions.
     | Drill down to see the time spent on every line of code.
    Memory Access: The Memory Bound metric is high  (67.4%). A significant fraction of execution pipeline slots could be stalled due to demand memory load and stores. 
     | Use Memory Access analysis to measure metrics that can identify memory
     | access issues.
    Threading: There is poor utilization of logical CPU cores (78.6%) in your application. 
     |  Use Threading to explore more opportunities to increase parallelism in
     | your application.

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
vtune: Executing actions 100 % Generating a report                             vtune: Executing actions 100 % done                                            
